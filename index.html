<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts">
  <meta name="keywords" content="Video-Text Retrieval, Multimodal, Benchmark, Long Video">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LoVR: A Benchmark for Long Video Retrieval</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/leaderboard.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script defer src="./static/js/sort-table.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
</head>

<body>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      updateRowNumbers("#retrieval-results");

      document.querySelector("#retrieval-results").addEventListener("click", function () {
        updateRowNumbers("#retrieval-results");
      });
    });

    function updateRowNumbers(tableId) {
      const rows = document.querySelectorAll(`${tableId} tbody tr`);
      rows.forEach((row, index) => {
        row.querySelector("td").innerText = index + 1;
      });
    }
  </script>

  <style>
    .no-sort {
      cursor: default;
      pointer-events: none;
      background-image: none !important;
    }
  </style>

  <!-- Hero Header Section. -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-3 publication-title">LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a>Qifeng Cai</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Hao Liang</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Zhaoyang Han</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Hejun Dong</a><sup style="color: #ed4b82;"></sup>,</span>
              <span class="author-block">
                <a>Meiyi Qiang</a><sup style="color: #6fbf73;"></sup>,</span>
              <br>
              <span class="author-block">
                <a>Ruichuan An</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Zhengzhou Zhu</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Bin Cui</a><sup style="color: #6fbf73;"></sup>,</span>
              <span class="author-block">
                <a>Wentao Zhang</a><sup style="color: #6fbf73;"></sup></span>
            </div>
            <br>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2505.13928" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/TechNomad-ds/LoVR-benchmark" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/debugger123/LoVR-benchmark" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- Leaderboard Link. -->
                <span class="link-block">
                  <a href="#leaderboard" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-trophy"></i>
                    </span>
                    <span>Leaderboard</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="./static/images/face.png" alt="comparison of captions" />
      </div>
    </div>
  </section>

  <!-- Abstract Section. -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Long videos contain a vast amount of information, making video-text retrieval an essential and challenging task in multimodal learning and web-scale search. On today's Web, where users increasingly expect to locate not only relevant pages but also specific long videos or fine-grained clips, existing benchmarks fall short due to limited video duration, low-quality captions, and coarse annotation granularity.
            </p>
            <p>
              To address these limitations, we introduce <b>LoVR</b>, a benchmark specifically designed for long video-text retrieval. <b>LoVR</b> contains <b>467 long videos</b> and over <b>40,804 fine-grained clips</b> with high-quality captions.
              To overcome the issue of poor machine-generated annotations, we propose an efficient caption generation framework that integrates VLM automatic generation, caption quality scoring, and dynamic refinement. This pipeline improves annotation accuracy while maintaining scalability. Furthermore, we introduce a semantic fusion method to generate coherent full-video captions without losing important contextual information.
            </p>
            <p>
              Our benchmark introduces longer videos, more detailed captions, and a larger-scale dataset, presenting new challenges for video understanding and retrieval. Extensive experiments on various advanced models demonstrate that <b>LoVR</b> is challenging, revealing the limitations of current approaches and providing valuable insights for future research.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section. -->
  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Introduction</h2>
          <div class="content has-text-justified">
            <p>
              ðŸŒŸ <b>LoVR</b> is a novel benchmark specifically designed for <b>long video-text retrieval</b>, addressing key limitations in existing datasets such as short video duration, low-quality captions, and insufficient annotation granularity.
            </p>
            <img src="./static/images/overview.png" alt="LoVR Pipeline Overview" />
            <p class="has-text-centered" style="font-size: 14px; color: #666; margin-top: 10px;">
              Overview of the data construction pipeline in <b>LoVR</b>. Step 1 segments long videos into high-dynamic clips. Step 2 generates high-quality clip-level captions via iterative VLM captioning. Step 3 constructs long-video captions by clustering and summarizing clip captions.
            </p>
            <hr style="border: 0; height: 1px; background-color: #f0f0f0; margin: 30px 0;">
            <p>
              ðŸ“Š <b>Key Features:</b>
            </p>
            <ul>
              <li><b>Long Videos:</b> 467 videos exceeding 15 minutes (avg. ~26 min), totaling over 200 hours</li>
              <li><b>Fine-grained Clips:</b> 40,804 clips with high-quality captions</li>
              <li><b>Rich Annotations:</b> Both clip-level and video-level captions with scene, atmosphere, and theme information</li>
              <li><b>Scalable Pipeline:</b> VLM + Human hybrid annotation ensuring quality and efficiency</li>
            </ul>
            <hr style="border: 0; height: 1px; background-color: #f0f0f0; margin: 30px 0;">
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-widescreen">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Leaderboard</h2>
          <div class="content has-text-justified">

  <!-- Comparison Table Section -->
  <img src="./static/images/leaderboard1.png" alt="LoVR Leaderboard1" />
  <p class="has-text-centered" style="font-size: 14px; color: #666; margin-top: 10px;">
    Baseline performance of text-to-video and text-to-clip retrieval on the LoVR benchmark.
  </p>
  <hr style="border: 0; height: 1px; background-color: #f0f0f0; margin: 30px 0;">
  <p>

    <img src="./static/images/leaderboard2.png" alt="LoVR Leaderboard2" />
    <p class="has-text-centered" style="font-size: 14px; color: #666; margin-top: 10px;">
      Baseline performance of video-to-text and clip-to-text retrieval on the LoVR benchmark.
    </p>
    <hr style="border: 0; height: 1px; background-color: #f0f0f0; margin: 30px 0;">
    <p>

    </div>
  </div>
</div>
</div>
</section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{cai2025lovr,
        title={LoVR: A Benchmark for Long Video Retrieval in Multimodal Contexts},
        author={Cai, Qifeng and Liang, Hao and Dong, Hejun and Qiang, Meiyi and An, Ruichuan and Han, Zhaoyang and Zhu, Zhengzhou and Cui, Bin and Zhang, Wentao},
        journal={arXiv preprint arXiv:2505.13928},
        year={2025}
      }
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://arxiv.org/pdf/2505.13928">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/TechNomad-ds/LoVR-benchmark" class="external-link">
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              This website is based on the template from <a
                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. We sincerely thank them for their
              great work.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>
